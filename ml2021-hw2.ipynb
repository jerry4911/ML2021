{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Homework 2-1: Phoneme Classification**\n\n* Slides: https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/hw/HW02/HW02.pdf\n* Video (Chinese): https://youtu.be/PdjXnQbu2zo\n* Video (English): https://youtu.be/ESRr-VCykBs","metadata":{"id":"Q3n38jXx6rIv"}},{"cell_type":"markdown","source":"## **The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)**\nThe TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\n\nThis homework is a multiclass classification task, \nwe are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\n\nlink: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3","metadata":{"id":"YRHCSR4g_dxu"}},{"cell_type":"markdown","source":"## **Download Data**\nDownload data from google drive, then unzip it.\n\nYou should have `timit_11/train_11.npy`, `timit_11/train_label_11.npy`, and `timit_11/test_11.npy` after running this block.<br><br>\n`timit_11/`\n- `train_11.npy`: training data<br>\n- `train_label_11.npy`: training label<br>\n- `test_11.npy`:  testing data<br><br>\n\n**notes: if the google drive link is dead, you can download the data directly from Kaggle and upload it to the workspace**","metadata":{"id":"YBHlTX2C_jso"}},{"cell_type":"markdown","source":"因為kaggle有了,所以不用載~","metadata":{}},{"cell_type":"code","source":"'''\n!gdown --id '1HPkcmQmFGu-3OknddKIa5dNDsR05lIQR' --output data.zip\n!unzip data.zip\n!ls \n'''","metadata":{"id":"vMld_fdj_6NC","outputId":"50494452-8103-49ff-db0f-d3161dc3c970","execution":{"iopub.status.busy":"2021-07-17T16:30:08.254977Z","iopub.execute_input":"2021-07-17T16:30:08.255345Z","iopub.status.idle":"2021-07-17T16:30:08.267425Z","shell.execute_reply.started":"2021-07-17T16:30:08.255264Z","shell.execute_reply":"2021-07-17T16:30:08.266608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Preparing Data**\nLoad the training and testing data from the `.npy` file (NumPy array).","metadata":{"id":"qdSzF7SN_9R-"}},{"cell_type":"code","source":"import numpy as np\n\nprint('Loading data ...')\n\n# data_root='./timit_11/'\ndata_root = '../input/ml2021spring-hw2/timit_11/timit_11/'\ntrain = np.load(data_root + 'train_11.npy')\ntrain_label = np.load(data_root + 'train_label_11.npy')\ntest = np.load(data_root + 'test_11.npy')\n\n# In numpy, V.shape gives a tuple of ints of dimensions of V\n# In pytorch, V.size() gives a size object\nprint('Size of training data: {}'.format(train.shape))\nprint('Size of training_label data: {}'.format(train_label.shape))\nprint('Size of testing data: {}'.format(test.shape))\n'''\nprint('Size of training data: {}'.format(train.size)) # 所有 data量\nprint('Size of training_label data: {}'.format(train_label.size))\nprint('Size of testing data: {}'.format(test.size))\n'''\n\n# 我覺得需要研究一下裡面到底有什麼QQ\n# 先取出第一個 11X39的data,代表11個frame,以及對應的39個 phoneum\n# phoneum是不是就代表對應的相關程度??\n# 看看正中間那個frame是不是真的和label標示的那個類別的關聯度最高\n# print('train的第0項',train[0])\nprint(train_label[0])\n# print('test的第0項',test[0])\ntrain_center_frame = train[0].reshape(11,39)\ntrain_center_frame = train_center_frame[5]\nprint('其中label對應的應該是正中間的那個frame, 所以是',train_center_frame)","metadata":{"id":"Psi762pvABY5","outputId":"20a1bf9c-f226-43ba-c162-c2e47b29ef8a","execution":{"iopub.status.busy":"2021-07-17T16:30:08.274656Z","iopub.execute_input":"2021-07-17T16:30:08.275149Z","iopub.status.idle":"2021-07-17T16:30:55.63728Z","shell.execute_reply.started":"2021-07-17T16:30:08.275112Z","shell.execute_reply":"2021-07-17T16:30:55.636266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"所以就是總共有1229932筆 data, 每個 data都是(11,39)\n\n* 11是frame的數量, 而 label是正中間那個frame\n* 39是 MFCC的維度,代表 39種發音的one-hot vector\n\n那麼 train_label 是之後才要幫忙取出來的意思? label直接就是那個 11frame中的正中間的frame的類別\n助教投影片說 train_label是0-38的 發音label","metadata":{}},{"cell_type":"markdown","source":"## **Create Dataset**","metadata":{"id":"kvmX5ho9AEbk"}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\n# 自己設定 dataset\nclass TIMITDataset(Dataset):\n    def __init__(self, X, y=None):\n        self.data = torch.from_numpy(X).float()\n        if y is not None:\n            y = y.astype(np.int)\n            self.label = torch.LongTensor(y)\n        else:\n            self.label = None\n\n    def __getitem__(self, idx):\n        if self.label is not None:\n            return self.data[idx], self.label[idx]\n        else:\n            return self.data[idx]\n\n    def __len__(self):\n        return len(self.data)","metadata":{"id":"iKDByub5AIrh","execution":{"iopub.status.busy":"2021-07-17T16:30:55.639079Z","iopub.execute_input":"2021-07-17T16:30:55.639615Z","iopub.status.idle":"2021-07-17T16:30:56.878461Z","shell.execute_reply.started":"2021-07-17T16:30:55.639575Z","shell.execute_reply":"2021-07-17T16:30:56.8776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split the labeled data into a training set and a validation set, you can modify the variable `VAL_RATIO` to change the ratio of validation data.","metadata":{"id":"HVixUbreAMg7"}},{"cell_type":"code","source":"VAL_RATIO = 0.05\n\npercent = int(train.shape[0] * (1 - VAL_RATIO))\n# x表示 data, y表示label\n# TODO? 要不要再 shuffle看看?\ntrain_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\nprint('Size of training set: {}'.format(train_x.shape))\nprint('Size of validation set: {}'.format(val_x.shape))","metadata":{"id":"zmVzo1gSAO9m","outputId":"42d9ae51-28ad-4773-e756-3b0e23b1ab48","execution":{"iopub.status.busy":"2021-07-17T16:30:56.882133Z","iopub.execute_input":"2021-07-17T16:30:56.882402Z","iopub.status.idle":"2021-07-17T16:30:56.890725Z","shell.execute_reply.started":"2021-07-17T16:30:56.882374Z","shell.execute_reply":"2021-07-17T16:30:56.889522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a data loader from the dataset, feel free to tweak the variable `BATCH_SIZE` here.","metadata":{"id":"hGPymvhVARtD"}},{"cell_type":"code","source":"BATCH_SIZE = 64\n\nfrom torch.utils.data import DataLoader\n\ntrain_set = TIMITDataset(train_x, train_y) # 丟進自己的 dataset去處理\nval_set = TIMITDataset(val_x, val_y)\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\nval_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"id":"vU3ZnrtsAUXZ","execution":{"iopub.status.busy":"2021-07-17T16:30:56.89234Z","iopub.execute_input":"2021-07-17T16:30:56.892677Z","iopub.status.idle":"2021-07-17T16:30:59.339366Z","shell.execute_reply.started":"2021-07-17T16:30:56.89264Z","shell.execute_reply":"2021-07-17T16:30:59.338446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cleanup the unneeded variables to save memory.<br>\n\n**notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later<br>the data size is quite huge, so be aware of memory usage in colab**","metadata":{"id":"muE_rDxJAWVV"}},{"cell_type":"code","source":"import gc\n\ndel train, train_label, train_x, train_y, val_x, val_y\ngc.collect()","metadata":{"id":"agecgDrQAbSJ","outputId":"011db7ab-0c45-4851-b28b-40a5c057d0ee","execution":{"iopub.status.busy":"2021-07-17T16:30:59.341015Z","iopub.execute_input":"2021-07-17T16:30:59.341396Z","iopub.status.idle":"2021-07-17T16:30:59.438521Z","shell.execute_reply.started":"2021-07-17T16:30:59.341361Z","shell.execute_reply":"2021-07-17T16:30:59.437375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Create Model**\n\nDefine model architecture, you are encouraged to change and experiment with the model architecture.","metadata":{"id":"mYiYiou3AeHB"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        # TODO? 設一下dropout\n        # self.Dropout = True # 怎麼好像沒用??\n        self.layer1 = nn.Linear(429, 1024)\n        self.layer2 = nn.Linear(1024, 512)\n        self.layer3 = nn.Linear(512, 256)\n        self.layer4 = nn.Linear(256, 128)\n        self.layer5 = nn.Linear(128, 64)\n        self.out = nn.Linear(64, 39) \n        # TODO? 要加上 Batch normalization的 layer\n        \n        self.batchNorm1 = nn.BatchNorm1d(1024)\n        self.batchNorm2 = nn.BatchNorm1d(512)\n        self.batchNorm3 = nn.BatchNorm1d(256)\n        self.batchNorm4 = nn.BatchNorm1d(128)\n        self.batchNorm5 = nn.BatchNorm1d(64)\n        # self.batchNorm = nn.BatchNorm1d(11)\n\n        self.act_fn = nn.Sigmoid()\n\n    def forward(self, x):\n        # TODO? 加一下 Batch normalization\n        # 每次input是 batchsize x 11 x 39\n        # batch_size =  int(x.size)//429\n        '''\n        x_bn = x.reshape(-1,11,39)\n        x = self.batchNorm(x_bn)\n        x = x.reshape(-1,429)\n        '''\n        x = self.layer1(x)\n        x = self.batchNorm1(x)\n        x = self.act_fn(x)\n\n        x = self.layer2(x)\n        x = self.batchNorm2(x)\n        x = self.act_fn(x)\n\n        x = self.layer3(x)\n        x = self.batchNorm3(x)\n        x = self.act_fn(x)\n        \n        x = self.layer4(x)\n        x = self.batchNorm4(x)\n        x = self.act_fn(x)\n        \n        x = self.layer5(x)\n        x = self.batchNorm5(x)\n        x = self.act_fn(x)\n        \n        # TODO? 輸出層可以改成softmax看看\n        # 變成多一層softmax\n        x = self.out(x)\n        \n        return x","metadata":{"id":"JRQtw1XCAk3j","execution":{"iopub.status.busy":"2021-07-17T16:30:59.440329Z","iopub.execute_input":"2021-07-17T16:30:59.440953Z","iopub.status.idle":"2021-07-17T16:30:59.455208Z","shell.execute_reply.started":"2021-07-17T16:30:59.440892Z","shell.execute_reply":"2021-07-17T16:30:59.454319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Training**","metadata":{"id":"VRkWMd2oAnzS"}},{"cell_type":"code","source":"#check device\ndef get_device():\n  return 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"id":"ixS4oLwLAq6r","execution":{"iopub.status.busy":"2021-07-17T16:30:59.456472Z","iopub.execute_input":"2021-07-17T16:30:59.456821Z","iopub.status.idle":"2021-07-17T16:30:59.468961Z","shell.execute_reply.started":"2021-07-17T16:30:59.456785Z","shell.execute_reply":"2021-07-17T16:30:59.468217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fix random seeds for reproducibility.","metadata":{"id":"aMzUASgMAtw1"}},{"cell_type":"code","source":"# fix random seed\ndef same_seeds(seed):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)  \n    np.random.seed(seed)  \n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True","metadata":{"id":"L9VXoUVQAwNf","execution":{"iopub.status.busy":"2021-07-17T16:30:59.471961Z","iopub.execute_input":"2021-07-17T16:30:59.472222Z","iopub.status.idle":"2021-07-17T16:30:59.480318Z","shell.execute_reply.started":"2021-07-17T16:30:59.472198Z","shell.execute_reply":"2021-07-17T16:30:59.479369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feel free to change the training parameters here.","metadata":{"id":"XlxD8gfYAySB"}},{"cell_type":"code","source":"# fix random seed for reproducibility\nsame_seeds(0)\n\n# get device \ndevice = get_device()\nprint(f'DEVICE: {device}')\n\n# training parameters\nnum_epoch = 20               # number of training epoch\nlearning_rate = 0.0001       # learning rate\nweight_decay = 1e-5\n\n# the path where checkpoint saved\nmodel_path = './model.ckpt'\n\n# create model, define a loss function, and optimizer\nmodel = Classifier().to(device)\ncriterion = nn.CrossEntropyLoss() \noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)","metadata":{"id":"QlBQm29QA0NE","outputId":"93d38ace-fff7-4a1e-82a6-8f60be468280","execution":{"iopub.status.busy":"2021-07-17T16:30:59.483068Z","iopub.execute_input":"2021-07-17T16:30:59.483513Z","iopub.status.idle":"2021-07-17T16:31:03.947426Z","shell.execute_reply.started":"2021-07-17T16:30:59.48341Z","shell.execute_reply":"2021-07-17T16:31:03.946506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start training\n\nbest_acc = 0.0\n# best_loss = 100000.0 #不是應該用validation的loss來看嗎?? 好像沒有比較好\nfor epoch in range(num_epoch):\n    train_acc = 0.0\n    train_loss = 0.0\n    val_acc = 0.0\n    val_loss = 0.0\n\n    # training\n    model.train() # set the model to training mode\n    for i, data in enumerate(train_loader):\n        inputs, labels = data # 包括 11x39 和 label\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad() \n        outputs = model(inputs) \n        # TODO? 為什麼不用 crossentropy\n        # 助教是用 crossentropy沒錯\n        batch_loss = criterion(outputs, labels) \n        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n        batch_loss.backward() \n        optimizer.step() \n\n        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n        train_loss += batch_loss.item()\n\n    # validation\n    if len(val_set) > 0:\n        model.eval() # set the model to evaluation mode\n        with torch.no_grad():\n            for i, data in enumerate(val_loader):\n                inputs, labels = data\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                batch_loss = criterion(outputs, labels) \n                _, val_pred = torch.max(outputs, 1) \n            \n                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n                val_loss += batch_loss.item()\n\n            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n            ))\n\n            # if the model improves, save a checkpoint at this epoch    \n            \n            if val_acc > best_acc:\n                best_acc = val_acc\n                torch.save(model.state_dict(), model_path)\n                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n            '''\n            if val_loss < best_loss:\n                best_loss = val_loss\n                torch.save(model.state_dict(), model_path)\n                print('saving model with loss {:.6f}'.format(best_loss/len(val_loader)))   \n            '''\n    else:\n        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n        ))\n\n# if not validating, save the last epoch\nif len(val_set) == 0:\n    torch.save(model.state_dict(), model_path)\n    print('saving model at last epoch')","metadata":{"id":"rSmFTvnOA2_5","outputId":"1451479b-76db-46c0-889c-75bf971ae3d8","execution":{"iopub.status.busy":"2021-07-17T16:31:03.949004Z","iopub.execute_input":"2021-07-17T16:31:03.949383Z","iopub.status.idle":"2021-07-17T17:13:59.447267Z","shell.execute_reply.started":"2021-07-17T16:31:03.949338Z","shell.execute_reply":"2021-07-17T17:13:59.446325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Testing**\n\nCreate a testing dataset, and load model from the saved checkpoint.","metadata":{"id":"Xk6Lk5PfA5p_"}},{"cell_type":"code","source":"# create testing dataset\ntest_set = TIMITDataset(test, None)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n\n# create model and load weights from checkpoint\nmodel = Classifier().to(device)\nmodel.load_state_dict(torch.load(model_path))","metadata":{"id":"6tc_rxaAA-YV","outputId":"8cbea399-0f15-4b58-9d1d-6c2efc3e9de8","execution":{"iopub.status.busy":"2021-07-17T17:13:59.448565Z","iopub.execute_input":"2021-07-17T17:13:59.449104Z","iopub.status.idle":"2021-07-17T17:13:59.991974Z","shell.execute_reply.started":"2021-07-17T17:13:59.449063Z","shell.execute_reply":"2021-07-17T17:13:59.991027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Make prediction.","metadata":{"id":"W9noIIAoBAxP"}},{"cell_type":"code","source":"predict = []\nmodel.eval() # set the model to evaluation mode\nwith torch.no_grad():\n    for i, data in enumerate(test_loader):\n        inputs = data\n        inputs = inputs.to(device)\n        outputs = model(inputs)\n        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n\n        for y in test_pred.cpu().numpy():\n            predict.append(y)","metadata":{"id":"3Ycgz9KIBCYB","execution":{"iopub.status.busy":"2021-07-17T17:13:59.993383Z","iopub.execute_input":"2021-07-17T17:13:59.993748Z","iopub.status.idle":"2021-07-17T17:14:08.407511Z","shell.execute_reply.started":"2021-07-17T17:13:59.993709Z","shell.execute_reply":"2021-07-17T17:14:08.406682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Write prediction to a CSV file.\n\nAfter finish running this block, download the file `prediction.csv` from the files section on the left-hand side and submit it to Kaggle.","metadata":{"id":"fG9dSSnIBEsw"}},{"cell_type":"code","source":"with open('prediction.csv', 'w') as f:\n    f.write('Id,Class\\n')\n    for i, y in enumerate(predict):\n        f.write('{},{}\\n'.format(i, y))","metadata":{"id":"diroa7PaBGR-","execution":{"iopub.status.busy":"2021-07-17T17:14:08.408718Z","iopub.execute_input":"2021-07-17T17:14:08.409072Z","iopub.status.idle":"2021-07-17T17:14:08.800384Z","shell.execute_reply.started":"2021-07-17T17:14:08.409036Z","shell.execute_reply":"2021-07-17T17:14:08.799495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Hints**\n\n## **Simple baseline**\n* You should able to pass the simple baseline using the sample code provided.\n\n## **Strong baseline**\n* Model architecture (layers? dimension? activation function?)\n* Training (batch size? optimizer? learning rate? epoch?)\n* Tips (batch norm? dropout? regularization?)","metadata":{"id":"oQ0Q_PHrIM2Q"}}]}